{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339ad88c-a203-48e7-9808-4a5680176085",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, StringType, LongType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from datetime import datetime\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Configuration and Setup\n",
    "# Define the source catalog and schema for the Silver layer\n",
    "source_catalog = \"silver\"\n",
    "source_schema = \"e-commerce-sales\"\n",
    "\n",
    "# Define the target Unity Catalog and schema for the Gold layer\n",
    "target_catalog = \"gold\"\n",
    "target_schema = \"e-commerce-sales\"\n",
    "target_table = \"daily_sales_metrics\"\n",
    "audit_log_table = \"audit_logs\"\n",
    "\n",
    "\n",
    "print(f\"Source: '{source_catalog}.{source_schema}'\")\n",
    "print(f\"Destination: '{target_catalog}.{target_schema}.{target_table}'\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Audit Logging Function\n",
    "audit_log_table_full_name = f\"{target_catalog}.`{target_schema}`.`{audit_log_table}`\"\n",
    "\n",
    "def log_audit(process_name, status, row_count=None, message=None):\n",
    "    \"\"\"\n",
    "    Logs a record to the Gold layer audit_logs table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log_schema = StructType([\n",
    "            StructField(\"timestamp\", TimestampType(), False),\n",
    "            StructField(\"process_name\", StringType(), False),\n",
    "            StructField(\"status\", StringType(), False),\n",
    "            StructField(\"row_count\", LongType(), True),\n",
    "            StructField(\"message\", StringType(), True)\n",
    "        ])\n",
    "        \n",
    "        log_data = [(datetime.now(), process_name, status, row_count, message)]\n",
    "        log_df = spark.createDataFrame(log_data, schema=log_schema)\n",
    "        log_df.write.format(\"delta\").mode(\"append\").saveAsTable(audit_log_table_full_name)\n",
    "        print(f\"Logged '{status}' for process '{process_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FATAL: Could not write to audit log table. Error: {e}\")\n",
    "\n",
    "# Log the start of the entire Gold job\n",
    "log_audit(\"Gold Layer Job\", \"Started\", message=\"Gold layer aggregation job initiated.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Load Silver Tables\n",
    "process_name = \"Load Silver Tables\"\n",
    "try:\n",
    "    log_audit(process_name, \"Started\")\n",
    "    orders_df = spark.table(f\"{source_catalog}.`{source_schema}`.orders\")\n",
    "    order_items_df = spark.table(f\"{source_catalog}.`{source_schema}`.order_items\")\n",
    "    users_df = spark.table(f\"{source_catalog}.`{source_schema}`.users\")\n",
    "    products_df = spark.table(f\"{source_catalog}.`{source_schema}`.products\")\n",
    "    log_audit(process_name, \"Success\")\n",
    "except Exception as e:\n",
    "    log_audit(process_name, \"Failed\", message=str(e))\n",
    "    dbutils.notebook.exit(f\"Failed to load source Silver tables. Details: {e}\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Join and Enrich Data\n",
    "process_name = \"Enrichment and Joins\"\n",
    "try:\n",
    "    log_audit(process_name, \"Started\")\n",
    "    # Join orders with order_items on the order ID\n",
    "    sales_df = orders_df.join(\n",
    "        order_items_df,\n",
    "        orders_df.order_id == order_items_df.order_id,\n",
    "        \"inner\"\n",
    "    ).select(\n",
    "        orders_df.order_id,\n",
    "        orders_df.created_at.alias(\"order_date\"),\n",
    "        orders_df.user_id,\n",
    "        order_items_df.product_id,\n",
    "        order_items_df.sale_price\n",
    "    )\n",
    "\n",
    "    # Join with products to get product details like cost and category\n",
    "    sales_with_products_df = sales_df.join(\n",
    "        products_df,\n",
    "        sales_df.product_id == products_df.id,\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        sales_df[\"*\"],\n",
    "        products_df.cost,\n",
    "        products_df.category.alias(\"product_category\")\n",
    "    )\n",
    "\n",
    "    # Join with users to get customer details like state\n",
    "    final_df = sales_with_products_df.join(\n",
    "        users_df,\n",
    "        sales_with_products_df.user_id == users_df.id,\n",
    "        \"left\"\n",
    "    ).select(\n",
    "        F.to_date(\"order_date\").alias(\"date\"),\n",
    "        sales_with_products_df[\"*\"],\n",
    "        users_df.state.alias(\"customer_state\")\n",
    "    )\n",
    "\n",
    "    # Add a profit column\n",
    "    final_df = final_df.withColumn(\"profit\", F.col(\"sale_price\") - F.col(\"cost\"))\n",
    "    log_audit(process_name, \"Success\")\n",
    "except Exception as e:\n",
    "    log_audit(process_name, \"Failed\", message=str(e))\n",
    "    dbutils.notebook.exit(f\"Failed during data enrichment. Details: {e}\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Aggregate Metrics for Gold Table\n",
    "process_name = \"Aggregation\"\n",
    "try:\n",
    "    log_audit(process_name, \"Started\")\n",
    "    daily_sales_metrics = final_df.groupBy(\"date\", \"customer_state\", \"product_category\").agg(\n",
    "        F.sum(\"sale_price\").alias(\"total_revenue\"),\n",
    "        F.sum(\"cost\").alias(\"total_cost\"),\n",
    "        F.sum(\"profit\").alias(\"total_profit\"),\n",
    "        F.count(\"order_id\").alias(\"items_sold_count\"),\n",
    "        F.countDistinct(\"order_id\").alias(\"distinct_order_count\"),\n",
    "        F.countDistinct(\"product_id\").alias(\"distinct_products_sold\")\n",
    "    ).cache() # Cache the result as it will be used for count and write\n",
    "\n",
    "    # Get the final count for logging\n",
    "    final_row_count = daily_sales_metrics.count()\n",
    "    log_audit(process_name, \"Success\", row_count=final_row_count)\n",
    "except Exception as e:\n",
    "    log_audit(process_name, \"Failed\", message=str(e))\n",
    "    dbutils.notebook.exit(f\"Failed during aggregation. Details: {e}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Write to Gold Layer\n",
    "process_name = \"Write to Gold Table\"\n",
    "gold_table_full_name = f\"{target_catalog}.`{target_schema}`.`{target_table}`\"\n",
    "\n",
    "try:\n",
    "    log_audit(process_name, \"Started\", row_count=final_row_count, message=f\"Attempting to write to {gold_table_full_name}\")\n",
    "    daily_sales_metrics.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(gold_table_full_name)\n",
    "    \n",
    "    log_audit(process_name, \"Success\", row_count=final_row_count, message=f\"Successfully wrote {final_row_count} rows.\")\n",
    "\n",
    "except Exception as e:\n",
    "    log_audit(process_name, \"Failed\", message=str(e))\n",
    "    dbutils.notebook.exit(f\"Failed to write to Gold table. Details: {e}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# DBTITLE 1,Finalize Job\n",
    "log_audit(\"Gold Layer Job\", \"Finished\", message=\"Gold layer aggregation job completed successfully.\")\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(f\"Preview of the final Gold table: {gold_table_full_name}\")\n",
    "display(spark.table(gold_table_full_name).orderBy(F.desc(\"date\"), F.desc(\"total_revenue\")).limit(20))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(f\"Preview of the Gold audit log: {audit_log_table_full_name}\")\n",
    "display(spark.table(audit_log_table_full_name).orderBy(F.desc(\"timestamp\")))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "gold2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
